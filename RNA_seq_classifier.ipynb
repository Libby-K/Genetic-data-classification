{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f0af0da7-ba23-4d51-a7ae-8ba9e10377cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b590938c-355a-48f5-b2a6-17a6017a7dd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import The data\n",
    "raw = pd.read_csv('features.txt', index_col = 0)\n",
    "features = pd.read_csv('labels.txt', index_col = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9da12a9e-a521-4ae9-b6d9-3ebf8f58d0da",
   "metadata": {},
   "source": [
    "### Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "71ad289d-cd5f-4e3b-80df-2a2e26abe32b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def explore(df):\n",
    "    \"\"\"\n",
    "    This function is used to explore a given pandas DataFrame and print out information such as the number of duplicated \n",
    "    rows, the shape of the DataFrame, the number of columns with NaNs, any rows containing NaNs, general information \n",
    "    about the DataFrame, and (optionally) descriptive statistics for numerical and object columns.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pandas DataFrame\n",
    "        The DataFrame to be explored.\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    None\n",
    "    \"\"\"\n",
    "    # Display the first two rows of the DataFrame\n",
    "    display(df.head(2))\n",
    "    \n",
    "    # Calculate and print the number of duplicated rows in the DataFrame\n",
    "    print(\"Number of duplicated rows: \", df.duplicated().sum())\n",
    "    \n",
    "    # Print the shape of the DataFrame\n",
    "    print(\"Data shape\", df.shape)\n",
    "    \n",
    "    # Identify any columns with NaNs and print the number of columns with NaNs\n",
    "    cols_with_nans = df.columns[df.isna().any()].tolist()\n",
    "    print('Number of columns with NaNs:', len(cols_with_nans))\n",
    "    \n",
    "    # If there are NaNs, display the rows that contain them in those columns\n",
    "    if len(cols_with_nans) > 0:\n",
    "        print('Data with NaNs:')\n",
    "        display(df[cols_with_nans][df[cols_with_nans].isnull().any(axis=1)])\n",
    "    \n",
    "    # Display general information about the DataFrame\n",
    "    display(df.info())\n",
    "    \n",
    "    # (Optional) Display descriptive statistics for numerical columns\n",
    "    #display(df.describe())\n",
    "    \n",
    "    # (Optional) Display descriptive statistics for object (i.e., non-numeric) columns\n",
    "    #display(df.describe(include='object'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "43a0490f-dc5c-4544-82ef-c51302d5db59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>...</th>\n",
       "      <th>57832</th>\n",
       "      <th>57846</th>\n",
       "      <th>57885</th>\n",
       "      <th>57887</th>\n",
       "      <th>57908</th>\n",
       "      <th>57929</th>\n",
       "      <th>57932</th>\n",
       "      <th>57935</th>\n",
       "      <th>57954</th>\n",
       "      <th>57969</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>SRR1146243</th>\n",
       "      <td>4.057059</td>\n",
       "      <td>2.805784</td>\n",
       "      <td>3.935262</td>\n",
       "      <td>3.866202</td>\n",
       "      <td>2.585520</td>\n",
       "      <td>2.244047</td>\n",
       "      <td>8.248893</td>\n",
       "      <td>5.426924</td>\n",
       "      <td>5.124292</td>\n",
       "      <td>4.481451</td>\n",
       "      <td>...</td>\n",
       "      <td>3.363534</td>\n",
       "      <td>4.917973</td>\n",
       "      <td>1.701393</td>\n",
       "      <td>0.283353</td>\n",
       "      <td>2.439566</td>\n",
       "      <td>2.496922</td>\n",
       "      <td>2.379835</td>\n",
       "      <td>2.538491</td>\n",
       "      <td>3.165511</td>\n",
       "      <td>1.947549</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SRR1146244</th>\n",
       "      <td>3.318046</td>\n",
       "      <td>-2.362307</td>\n",
       "      <td>3.907833</td>\n",
       "      <td>3.333466</td>\n",
       "      <td>2.263786</td>\n",
       "      <td>2.973915</td>\n",
       "      <td>5.948596</td>\n",
       "      <td>4.329128</td>\n",
       "      <td>4.749798</td>\n",
       "      <td>4.002509</td>\n",
       "      <td>...</td>\n",
       "      <td>2.807221</td>\n",
       "      <td>4.533203</td>\n",
       "      <td>-1.493196</td>\n",
       "      <td>1.191666</td>\n",
       "      <td>2.593325</td>\n",
       "      <td>0.309551</td>\n",
       "      <td>1.735091</td>\n",
       "      <td>1.758127</td>\n",
       "      <td>4.482234</td>\n",
       "      <td>-0.255909</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows Ã— 15326 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                   1         2         3         4         5         6  \\\n",
       "SRR1146243  4.057059  2.805784  3.935262  3.866202  2.585520  2.244047   \n",
       "SRR1146244  3.318046 -2.362307  3.907833  3.333466  2.263786  2.973915   \n",
       "\n",
       "                   7         8         9        10  ...     57832     57846  \\\n",
       "SRR1146243  8.248893  5.426924  5.124292  4.481451  ...  3.363534  4.917973   \n",
       "SRR1146244  5.948596  4.329128  4.749798  4.002509  ...  2.807221  4.533203   \n",
       "\n",
       "               57885     57887     57908     57929     57932     57935  \\\n",
       "SRR1146243  1.701393  0.283353  2.439566  2.496922  2.379835  2.538491   \n",
       "SRR1146244 -1.493196  1.191666  2.593325  0.309551  1.735091  1.758127   \n",
       "\n",
       "               57954     57969  \n",
       "SRR1146243  3.165511  1.947549  \n",
       "SRR1146244  4.482234 -0.255909  \n",
       "\n",
       "[2 rows x 15326 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of duplicated rows:  0\n",
      "Data shape (177, 15326)\n",
      "Number of columns with NaNs: 11\n",
      "Data with NaNs:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>286</th>\n",
       "      <th>287</th>\n",
       "      <th>288</th>\n",
       "      <th>289</th>\n",
       "      <th>290</th>\n",
       "      <th>291</th>\n",
       "      <th>292</th>\n",
       "      <th>294</th>\n",
       "      <th>295</th>\n",
       "      <th>296</th>\n",
       "      <th>297</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>SRR1146130</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SRR1146131</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SRR1146132</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SRR1146133</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            286  287  288  289  290  291  292  294  295  296  297\n",
       "SRR1146130  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN\n",
       "SRR1146131  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN\n",
       "SRR1146132  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN\n",
       "SRR1146133  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 177 entries, SRR1146243 to SRR1146086\n",
      "Columns: 15326 entries, 1 to 57969\n",
      "dtypes: float64(15321), int64(5)\n",
      "memory usage: 20.7+ MB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "explore(raw)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19ce8230-33e5-41bc-97bd-1be17573661a",
   "metadata": {},
   "source": [
    "There are 4 rows with missing 11 missing values, The dataset isnt big so I will impute the missing values with each column median"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "56039436-d4ed-477b-beeb-b0bb9f37992d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "lesional    94\n",
       "normal      83\n",
       "Name: target, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check if the target is balanced\n",
    "features['target'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73d70349-1875-4857-9560-551ef4238683",
   "metadata": {},
   "source": [
    "The target is balanced so Accuracy will be good metric"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abc4f76f-7886-4809-983d-91eed77bc592",
   "metadata": {},
   "source": [
    "### Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1c4fcd29-e1af-44e5-a1b9-a03f0927db54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess the target to be numerical\n",
    "features['target'] = features['target'].map({'lesional':1, 'normal':0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1ec3bd56-c6e5-4abc-aeae-c863690e73b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_high_variance_features(df, threshold=0.05):\n",
    "    \"\"\"\n",
    "    Selects features with high variance from a Pandas DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "    - df (pandas.DataFrame): The DataFrame to select features from.\n",
    "    - threshold (float): The minimum variance a feature must have to be kept.\n",
    "\n",
    "    Returns:\n",
    "    - list: The list of selected feature names.\n",
    "    \"\"\"\n",
    "\n",
    "    # Calculate the variances of each column\n",
    "    variances = df.var()\n",
    "\n",
    "    # Select only columns with variance greater than the threshold\n",
    "    selected_columns = variances[variances > threshold].index.tolist()\n",
    "\n",
    "    return selected_columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7fa824e0-9809-40d1-8c2a-f9f05ff1e010",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Joining the raw data and features dataframes\n",
    "raw_ = raw.join(features)\n",
    "\n",
    "# Splitting the raw_ dataframe into training and testing datasets\n",
    "# test_size parameter determines the proportion of the dataset to include in the test split (in this case, 20%)\n",
    "# random_state parameter is used to ensure reproducibility of the results\n",
    "# stratify parameter is used to maintain the proportion of the target variable in both training and testing datasets\n",
    "df_train, df_test = train_test_split(raw_, test_size=0.2, random_state=0, stratify=raw_.iloc[:, -1])\n",
    "\n",
    "# Creating separate dataframes for the features and target variables in the training and testing datasets\n",
    "X_train, X_test, y_train, y_test = df_train.drop(columns=[\"target\"]), df_test.drop(columns=[\"target\"]), df_train[\"target\"], df_test[\"target\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a527ce8b-21f1-41be-a0b9-e9efbeac907d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num of features with high variance:  14845\n"
     ]
    }
   ],
   "source": [
    "# Compute the median of each feature in the train data\n",
    "medians = X_train.median()\n",
    "\n",
    "# Fill in missing values in train data using median of corresponding feature\n",
    "filled_train_data = X_train.fillna(medians)\n",
    "\n",
    "# Fill in missing values in test data using median of corresponding feature from train data\n",
    "filled_test_data = X_test.fillna(medians)\n",
    "\n",
    "high_variance_features = select_high_variance_features(filled_train_data)\n",
    "print('Num of features with high variance: ', len(high_variance_features))\n",
    "\n",
    "X_train_high_variance = filled_train_data[high_variance_features]\n",
    "X_test_high_variance = filled_test_data[high_variance_features]\n",
    "\n",
    "sc_X = StandardScaler()\n",
    "X_train = sc_X.fit_transform(X_train_high_variance)\n",
    "X_test = sc_X.transform(X_test_high_variance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "600a71f9-aeda-4b7d-bbaf-21e67307f1f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_pca_components(data, variance_percentage):\n",
    "    \"\"\"\n",
    "    Finds the number of PCA components required to retain a specified percentage of the explained variance.\n",
    "\n",
    "    Parameters:\n",
    "    - data (numpy.ndarray): The input data to transform using PCA.\n",
    "    - variance_percentage (float): The percentage of the explained variance to retain.\n",
    "\n",
    "    Returns:\n",
    "    - int: The number of components required to retain the specified percentage of the explained variance.\n",
    "    \"\"\"\n",
    "\n",
    "    # Create a PCA object with all components\n",
    "    pca = PCA()\n",
    "\n",
    "    # Fit the PCA object to the data\n",
    "    pca.fit(data)\n",
    "\n",
    "    # Calculate the cumulative explained variance ratio\n",
    "    cum_var_ratio = np.cumsum(pca.explained_variance_ratio_)\n",
    "\n",
    "    # Find the index of the first component that explains at least the specified percentage of the variance\n",
    "    n_components = np.argmax(cum_var_ratio >= variance_percentage) + 1\n",
    "\n",
    "    return n_components\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "98a4975b-8f51-4b7d-815b-5514ca84af60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "72 components explain 95.0 % of the variance\n"
     ]
    }
   ],
   "source": [
    "# Specify the desired percentage of variance to be explained by the principal components\n",
    "variance_percentage = 0.95\n",
    "\n",
    "# Call the function find_pca_components to determine the number of principal components that explain the desired percentage of variance\n",
    "n_components = find_pca_components(X_train, variance_percentage)\n",
    "\n",
    "# Print the number of components and the percentage of variance they explain\n",
    "print(f'{n_components} components explain {variance_percentage*100} % of the variance')\n",
    "\n",
    "# Instantiate a PCA object with the specified number of principal components\n",
    "pca = PCA(n_components)\n",
    "\n",
    "# Fit the PCA object to the training data and transform the data into the new feature space\n",
    "X_train = pca.fit_transform(X_train)\n",
    "\n",
    "# Transform the test data into the new feature space using the PCA object fitted on the training data\n",
    "X_test = pca.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "68904d0d-d086-4680-98a8-984822b5a71d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.DataFrame(X_train, index = filled_train_data.index).join(y_train)\n",
    "df_test = pd.DataFrame(X_test, index = filled_test_data.index).join(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "729cdc17-2e7d-48ee-8e54-f807eb28e638",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pickle the df_train dataframe\n",
    "with open('df_train.pkl', 'wb') as f:\n",
    "    pickle.dump(df_train, f)\n",
    "\n",
    "# Pickle the df_test dataframe\n",
    "with open('df_test.pkl', 'wb') as f:\n",
    "    pickle.dump(df_test, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9acec883-e3c7-450b-9d82-6cc5a9fd32a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate dataframes called 'df_normal' and 'df_lesional'\n",
    "\n",
    "df_lesional = df_train[df_train['target'] == 1]\n",
    "df_normal = df_train[df_train['target'] == 0]\n",
    "\n",
    "# Define the function to create meta-samples\n",
    "def create_meta_samples(df, num_samples=5):\n",
    "    # Get the number of features\n",
    "    num_features = df.shape[1] - 1  # assuming the last column is the target\n",
    "\n",
    "    # Initialize an empty dataframe to store the meta-samples\n",
    "    meta_samples = pd.DataFrame()\n",
    "\n",
    "    # Loop over the number of samples\n",
    "    for i in range(num_samples):\n",
    "        # Compute the mean and standard deviation for each feature\n",
    "        means = df.iloc[:, :-1].mean()\n",
    "        stds = df.iloc[:, :-1].std()\n",
    "\n",
    "        # Generate a random sample of the same size as the original data\n",
    "        sample = pd.DataFrame(np.random.normal(means, stds, size=(1, num_features)),\n",
    "                              columns=df.columns[:-1])\n",
    "\n",
    "        # Add the target column from the original data\n",
    "        sample['target'] = df['target'].mean()\n",
    "\n",
    "        # Append the sample to the meta-samples dataframe\n",
    "        meta_samples = pd.concat([meta_samples, sample], ignore_index=True)\n",
    "\n",
    "    return meta_samples\n",
    "\n",
    "# Create meta-samples for the normal data\n",
    "df_normal_meta = create_meta_samples(df_normal, num_samples=5)\n",
    "\n",
    "# Create meta-samples for the lesional data\n",
    "df_lesional_meta = create_meta_samples(df_lesional, num_samples=5)\n",
    "\n",
    "# Join the data together\n",
    "df_meta = pd.concat([df_normal_meta,df_lesional_meta])\n",
    "\n",
    "# Pickle the df_train_meta dataframe\n",
    "with open('df_train_meta.pkl', 'wb') as f:\n",
    "    pickle.dump(df_meta, f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3541c292-d47e-4360-bc1a-b7b24b1d8ac6",
   "metadata": {},
   "source": [
    "### Models traing and evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f8fbe51-2cfa-4a3f-a6db-ae8fcda69478",
   "metadata": {},
   "source": [
    "The training and evaluating py file contains 3 classifiers, lgbm, SVM and KNN:\n",
    "\n",
    "Two of them have internal regularization: \n",
    "\n",
    "* Support Vector Machines (SVMs): SVMs use regularization through the penalty parameter C that controls the trade-off between maximizing the margin and minimizing the classification error.\n",
    "\n",
    "* Gradient Boosting: This is another ensemble learning method that combines multiple weak learners to form a strong learner. Gradient boosting has internal regularization through the shrinkage parameter that controls the step size of the gradient descent optimization.\n",
    "\n",
    "The models are trained with the default parameters and with Baesian hyperparameter tuning and finaly all models both for basic and metadata data are compared in a table\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "edc5c257-d93a-4b4e-adc7-69c14c4d9194",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the Classifier trainer and selector\n",
    "import Model_selection_part_1_2 as cls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c774b122-159a-42c2-8239-7c7ee1bb9a56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:09<00:00,  1.08trial/s, best loss: -0.9787234042553191]\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6457920106999118, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6457920106999118\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5995676207447482, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5995676207447482\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:00<00:00, 23.57trial/s, best loss: -1.0]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:00<00:00, 12.11trial/s, best loss: -1.0]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:00<00:00, 25.05trial/s, best loss: -0.5]\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6290575566125438, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6290575566125438\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5301248279278932, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5301248279278932\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:00<00:00, 47.32trial/s, best loss: -1.0]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:00<00:00, 45.17trial/s, best loss: -1.0]\n"
     ]
    }
   ],
   "source": [
    "# Run the py file\n",
    "cls.main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "763306f8-9776-4c68-9e98-6d96d2973c97",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Data_type</th>\n",
       "      <th>Classifier</th>\n",
       "      <th>Accuracy Default score</th>\n",
       "      <th>Accuracy Bayesian score</th>\n",
       "      <th>Hyperparameters</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Basic</td>\n",
       "      <td>&lt;class 'lightgbm.sklearn.LGBMClassifier'&gt;</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>Default params</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Basic</td>\n",
       "      <td>&lt;class 'sklearn.svm._classes.SVC'&gt;</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>Default params</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Basic</td>\n",
       "      <td>&lt;class 'sklearn.neighbors._classification.KNei...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>Default params</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Meta</td>\n",
       "      <td>&lt;class 'lightgbm.sklearn.LGBMClassifier'&gt;</td>\n",
       "      <td>0.472222</td>\n",
       "      <td>0.472222</td>\n",
       "      <td>Default params</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Meta</td>\n",
       "      <td>&lt;class 'sklearn.svm._classes.SVC'&gt;</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.888889</td>\n",
       "      <td>Default params</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Meta</td>\n",
       "      <td>&lt;class 'sklearn.neighbors._classification.KNei...</td>\n",
       "      <td>0.916667</td>\n",
       "      <td>0.777778</td>\n",
       "      <td>Default params</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Data_type                                         Classifier  \\\n",
       "0     Basic          <class 'lightgbm.sklearn.LGBMClassifier'>   \n",
       "0     Basic                 <class 'sklearn.svm._classes.SVC'>   \n",
       "0     Basic  <class 'sklearn.neighbors._classification.KNei...   \n",
       "0      Meta          <class 'lightgbm.sklearn.LGBMClassifier'>   \n",
       "0      Meta                 <class 'sklearn.svm._classes.SVC'>   \n",
       "0      Meta  <class 'sklearn.neighbors._classification.KNei...   \n",
       "\n",
       "   Accuracy Default score  Accuracy Bayesian score Hyperparameters  \n",
       "0                1.000000                 1.000000  Default params  \n",
       "0                1.000000                 1.000000  Default params  \n",
       "0                1.000000                 1.000000  Default params  \n",
       "0                0.472222                 0.472222  Default params  \n",
       "0                1.000000                 0.888889  Default params  \n",
       "0                0.916667                 0.777778  Default params  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Present the comparrison\n",
    "comparisson = pd.read_csv('Comparisson.csv', index_col = 0)\n",
    "comparisson"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c05e23ee-8a44-4020-9cbc-2d10910ef502",
   "metadata": {},
   "source": [
    "### Discussion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3e4c5e0-9e0f-41bc-81c8-97ef0f94327f",
   "metadata": {},
   "source": [
    "#### Who performed better?\n",
    "\n",
    "In contrast to the models that were trained on basic data the preformance of the models trained on the metadata was less sucsessfull exept the SVM classifier with the default hiperparameters.\n",
    "\n",
    "There are a few possible reasons why the SVM classifier with default parameters may have performed better than the LGBM and KNN classifiers on metadata:\n",
    "\n",
    "* SVM is a powerful algorithm that can work well with high-dimensional data like metadata.\n",
    "* The default parameters of the SVM classifier may have been well-suited to the metadata being used.\n",
    "* The LGBM and KNN classifiers may not have been optimized or tuned for the specific metadata being used.\n",
    "* The LGBM or KNN classifiers may have been overfitting or underfitting the data.\n",
    "* The size and quality of the metadata used for training the classifiers could also play a role in determining their performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f478860d-1b37-4168-82a6-11f1bb782f0c",
   "metadata": {},
   "source": [
    "#### What are the potential implications of training on meta-samples?\n",
    "\n",
    "Training models on meta-samples can have potential privacy implications. When multiple smaller datasets are combined into a meta-sample, it can increase the risk of re-identification or other privacy breaches, particularly if the smaller datasets contain sensitive information."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f79ae98-d7ff-4322-a08e-8bc63ccdef4e",
   "metadata": {},
   "source": [
    "#### can you think of a way to use the meta-samples, and train better performing models?\n",
    "\n",
    "Preheaps it us possible to use these samples in transfer learning if a large dataset with these genes exsists. Another options is data augmentation and it is always possible to try different models or voting classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2110e43-40f3-4d7b-b5d9-5a53bf870a6a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
